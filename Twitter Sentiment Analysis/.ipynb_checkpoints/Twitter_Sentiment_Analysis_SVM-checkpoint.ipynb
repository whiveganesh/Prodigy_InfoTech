{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9e52eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-deep')\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12911941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the first dataset and shuffle the rows\n",
    "dataframe1 = pd.read_csv('twitter_validation.csv', encoding='latin1')\n",
    "dataframe1 = dataframe1.sample(frac=1)\n",
    "dataframe1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e020b6e6",
   "metadata": {},
   "source": [
    "# **Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e92329a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataframe1.columns[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273a525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe1 = dataframe1.rename(columns={'I mentioned on Facebook that I was struggling for motivation to go for a run the other day, which has been translated by Tomâs great auntie as âHayley canât get out of bedâ and told to his grandma, who now thinks Iâm a lazy, terrible person ð¤£':'text'})\n",
    "dataframe1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3223e04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c077b489",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe1.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d644499",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0aa2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap of missing values\n",
    "plt.figure(figsize=(10, 6))\n",
    "custom_cmap = sns.color_palette([\"#FF1493\", \"#D3D3D3\"])\n",
    "sns.heatmap(dataframe1.isnull(), cmap=custom_cmap, cbar=False, yticklabels=False)\n",
    "plt.title('Missing Values Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87db041a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_distribution_Facebook = dataframe1['Facebook'].value_counts()\n",
    "print(class_distribution_Facebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75f9e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming binary classification\n",
    "ratio_majority_minority_Facebook = class_distribution_Facebook[0] / class_distribution_Facebook[1]\n",
    "print(f\"Ratio between majority and minority classes: {ratio_majority_minority_Facebook}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cee228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot class distribution\n",
    "class_distribution_Facebook.plot(kind='bar', title='Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08d6a52",
   "metadata": {},
   "outputs": [],
   "source": [
    " class_distribution_Irrelevant = dataframe1['Irrelevant'].value_counts()\n",
    "print(class_distribution_Irrelevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912f13b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming binary classification\n",
    "ratio_majority_minority_Irrelevant = class_distribution_Irrelevant[0] / class_distribution_Irrelevant[1]\n",
    "print(f\"Ratio between majority and minority classes: {ratio_majority_minority_Irrelevant}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0000c24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot class distribution\n",
    "class_distribution_Irrelevant.plot(kind='bar', title='Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176de844",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(dataframe1[['Facebook']].values.ravel()) # set() fetch the unique data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251b7880",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe1.groupby('Facebook').Facebook.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82634ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(dataframe1['Irrelevant'].unique()) # set() fetch the unique data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b810b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe1.groupby('Irrelevant').Irrelevant.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4273f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyzing data\n",
    "dataframe1.groupby('Facebook').Facebook.count().plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d151ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyzing data\n",
    "dataframe1.groupby('Irrelevant').Irrelevant.count().plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94957ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Borderlands' and 'Positive' columns and count occurrences\n",
    "grouped_data = dataframe1.groupby(['Facebook', 'Irrelevant']).size().unstack()\n",
    "grouped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952cf176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Borderlands' and 'Positive' columns and count occurrences\n",
    "grouped_data = dataframe1.groupby(['Facebook', 'Irrelevant']).size().unstack()\n",
    "\n",
    "# Plotting\n",
    "grouped_data.plot(kind='bar', stacked=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f616f51",
   "metadata": {},
   "source": [
    "Let's make a new column to detect how long the text messages are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b28b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe1['Length'] = dataframe1['text'].apply(len)\n",
    "dataframe1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77c23b3",
   "metadata": {},
   "source": [
    "Let's see the percentage of ham and spam in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4050037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe1['Length'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0579937c",
   "metadata": {},
   "source": [
    "# **Text Cleaning**\n",
    "\n",
    "Let’s clean the text for the messages in our dataset with NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0fb32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22073dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mess = '''For me / @the love should start with attraction.i should feel that I need her every time around me.she should be the first thing which comes in my thoughts.I would start the day and end it with her.she should be there every time I dream.love will be then when my every breath has her name.my life should happen around her.my life will be named to her.I would cry for her.will give all my happiness and take all her sorrows.I will be ready to fight with anyone for her.I will be in love when I will be doing the craziest things for her.love will be when I don't have to proove anyone that my girl is the most beautiful lady on the whole planet.I will always be singing praises for her.love will be when I start up making chicken curry and end up makiing sambar.life will be the most beautiful then.will get every morning and thank god for the day because she is with me.I would like to say a lot..will tell later..'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800924eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nopunc = ''.join([char for char in mess if char not in string.punctuation])\n",
    "print(nopunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a7b6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a07bd84",
   "metadata": {},
   "source": [
    "Let's create the function to remove all punctuation, remove all stopwords and returns a list of the cleaned text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ff1a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(message):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = [char for char in message if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "    # Now just remove any stopwords\n",
    "    # split() convert data into list\n",
    "    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e74350e",
   "metadata": {},
   "source": [
    "**Vectorization**\n",
    "\n",
    "***CountVectorizer :*** CountVectorizer is a feature extraction technique used to convert a collection of text documents to a matrix of token counts.\n",
    "\n",
    "Now we have the messages as lists and we need to convert each of those messages into a vector that SciKit Learn's algorithm models can work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2955f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "## text_process function is calling by passing parameter dataframe1['text']\n",
    "bow_transformer = CountVectorizer(analyzer=text_process).fit(dataframe1['text'])\n",
    "bow_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a41851",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_transformer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2149af7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(bow_transformer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9458fc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "message4 = dataframe1['text'][3]\n",
    "print(message4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bde41c5",
   "metadata": {},
   "source": [
    "Now let's transform the entire DataFrame of messages and create sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e781d1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_bow = bow_transformer.transform(dataframe1['text'])\n",
    "messages_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c528d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of Sparse Matrix: ', messages_bow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b71e64",
   "metadata": {},
   "source": [
    "# **TF-IDF**\n",
    "\n",
    "Now let's compute term weighting and do normalisation with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b08c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer().fit(messages_bow)\n",
    "tfidf4 = tfidf_transformer.transform(messages_bow)\n",
    "print(tfidf4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9797d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidf4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa28ab38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# from datetime import datetime\n",
    "# start_time = datetime.now()\n",
    "\n",
    "# model = SVC()\n",
    "# model.fit(x_train,y_train)\n",
    "\n",
    "# end_time = datetime.now()\n",
    "# process_time = round(end_time-start_time,2)\n",
    "# print(\"Fitting SVC took {} seconds\".format(process_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e974c4e",
   "metadata": {},
   "source": [
    "**Train Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbf2ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "text_train, text_test, label_train, label_test = train_test_split(dataframe1['text'], dataframe1['Irrelevant'], test_size=0.2,shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e08f53",
   "metadata": {},
   "source": [
    "**Creating a Data Pipeline**\n",
    "\n",
    "Let's run our model again and then predict the test set. We will create and use a pipeline for this purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0076a72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer()),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier', SVC()),  # train on TF-IDF vectors w/ SVM      # accuracy = 0.54\n",
    "#     ('clf', BalancedRandomForestClassifier(n_estimators=100, random_state=42)) # Accuracy: 0.475\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c389bea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(text_train,label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25945b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_test.iloc[1:2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fe73de",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline.predict(text_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dbe464",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = np.array(['Irrelevant', 'Negative', 'Neutral', 'Positive'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57944214",
   "metadata": {},
   "source": [
    "# **Making Confusion Matrix**\n",
    "\n",
    "Confusion Matrix is going to contain the correct predictions that our model made on the set as well as the incorrect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deec541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "cm = confusion_matrix(label_test,predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a864b714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix and classification report(precision, recall, F1-score)\n",
    "# ytest = np.array(label_test)\n",
    "print(classification_report(pipeline.predict(text_test),label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1263bb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "class_names =['Irrelevant', 'Negative', 'Neutral', 'Positive']\n",
    "# Change figure size and increase dpi for better resolution\n",
    "# and get reference to axes object\n",
    "fig, ax = plt.subplots(figsize=(8,8), dpi=100)\n",
    "\n",
    "# initialize using the raw 2D confusion matrix\n",
    "# and output labels (in our case, it's 0 and 1)\n",
    "display = ConfusionMatrixDisplay(cm, display_labels=class_names)\n",
    "\n",
    "# set the plot title using the axes object\n",
    "ax.set(title='Confusion Matrix for the Diabetes Detection Model')\n",
    "\n",
    "# show the plot.\n",
    "# Pass the parameter ax to show customizations (ex. title)\n",
    "display.plot(ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d16f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(\"Accuracy:\",metrics.accuracy_score(predictions,label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af905ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open('/content/news.txt','r')\n",
    "# news = file.read()\n",
    "# file.close()\n",
    "\n",
    "sentiment = input(\"Enter sentiment = \")\n",
    "sentiment_data = {'predict_sentiment':[sentiment]}\n",
    "sentiment_data_df = pd.DataFrame(sentiment_data)\n",
    "\n",
    "predict_sentiment_cat = pipeline.predict(sentiment_data_df['predict_sentiment'])\n",
    "print(\"Predicted news category = \",predict_sentiment_cat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a38b383",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
